{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dd484c-b56c-430d-8a7a-cb8a81997326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df=spark.read.load(\"/Volumes/luffy/phase2/silver/test_df/\")\n",
    "train_df=spark.read.load(\"/Volumes/luffy/phase2/silver/train_df/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4872805f-a1a4-43cf-808d-2c1e57fc1434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5792462e-2ab9-43a3-8048-f3d2f6f23223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_high_valued\",\n",
    "    weightCol=\"class_weight\"\n",
    ")\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid_lr = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63b20c4-bba0-47e3-a202-4658788ef5d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual hyperparameter tuning (simple loop)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'regParam': 0.0, 'elasticNetParam': 0.0}, Best ROC AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "reg_params = [0.0, 0.01, 0.1]\n",
    "elastic_net_params = [0.0, 0.5, 1.0]\n",
    "\n",
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "best_params = None\n",
    "\n",
    "for reg in reg_params:\n",
    "    for enet in elastic_net_params:\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_high_valued\",\n",
    "                               regParam=reg, elasticNetParam=enet)\n",
    "        model = lr.fit(train_df)\n",
    "        predictions = model.transform(test_df)\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"is_high_valued\", metricName=\"areaUnderROC\")\n",
    "        score = evaluator.evaluate(predictions)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = {'regParam': reg, 'elasticNetParam': enet}\n",
    "\n",
    "print(f\"Best params: {best_params}, Best ROC AUC: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f7165c-e9ad-4ab0-9a92-cb4ac186a19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f4ade8-c14e-49de-81d7-8308db274140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'numTrees': 30, 'maxDepth': 5}, Best ROC AUC: 0.9997337375061446\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "num_trees = [10, 30, 60]\n",
    "max_depths = [5, 10, 20]\n",
    "\n",
    "best_rf_model = None\n",
    "best_rf_score = float('-inf')\n",
    "best_rf_params = None\n",
    "\n",
    "for ntrees in num_trees:\n",
    "    for depth in max_depths:\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"is_high_valued\",\n",
    "            numTrees=ntrees,\n",
    "            maxDepth=depth\n",
    "        )\n",
    "        rf_model = rf.fit(train_df)\n",
    "        rf_predictions = rf_model.transform(test_df)\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"is_high_valued\", metricName=\"areaUnderROC\")\n",
    "        rf_score = evaluator.evaluate(rf_predictions)\n",
    "        if rf_score > best_rf_score:\n",
    "            best_rf_score = rf_score\n",
    "            best_rf_model = rf_model\n",
    "            best_rf_params = {'numTrees': ntrees, 'maxDepth': depth}\n",
    "\n",
    "print(f\"Best params: {best_rf_params}, Best ROC AUC: {best_rf_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b91fd1d6-6450-4dea-a244-70dfbb8f4754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyperparameter tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}